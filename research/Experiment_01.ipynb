{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def download_kaggle_dataset():\n",
    "    artifacts_dir = \"../artifacts\"\n",
    "    \n",
    "    kaggle_url = \"https://www.kaggle.com/api/v1/datasets/download/sainikhileshreddy/food-recognition-2022\"\n",
    "    destination_path = os.path.join(artifacts_dir, \"food-recognition-2022.zip\")\n",
    "    \n",
    "    if not os.path.exists(artifacts_dir):\n",
    "        os.makedirs(artifacts_dir)\n",
    "        print(f\"Created directory: {artifacts_dir}\")\n",
    "    \n",
    "    try:\n",
    "        print(f\"Downloading dataset to {destination_path}...\")\n",
    "        subprocess.run(\n",
    "            [\"curl\", \"-L\", \"-o\", destination_path, kaggle_url],\n",
    "            check=True\n",
    "        )\n",
    "        print(\"Download completed successfully!\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error occurred while downloading the dataset: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_kaggle_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "def unzip_file(zip_file_path, extract_to_dir):\n",
    "    \"\"\"\n",
    "    Unzips a .zip file to the specified directory.\n",
    "    \n",
    "    Parameters:\n",
    "        zip_file_path (str): Path to the .zip file.\n",
    "        extract_to_dir (str): Directory where the files will be extracted.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(extract_to_dir):\n",
    "        os.makedirs(extract_to_dir)\n",
    "        print(f\"Created directory: {extract_to_dir}\")\n",
    "    \n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to_dir)\n",
    "            print(f\"Successfully extracted {zip_file_path} to {extract_to_dir}\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"Error: {zip_file_path} is not a valid zip file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    zip_file = \"../artifacts/food-recognition-2022.zip\"  \n",
    "    output_dir = \"../artifacts/FoodRecognition\"              \n",
    "    \n",
    "    unzip_file(zip_file, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_file_train = r'/Users/mac/Desktop/FoodRecommendation/artifacts/FoodRecognition/raw_data/public_training_set_release_2.0/annotations.json'\n",
    "annotations_file_val = r'/Users/mac/Desktop/FoodRecommendation/artifacts/FoodRecognition/raw_data/public_validation_set_2.0/annotations.json'\n",
    "image_file_train = r'/Users/mac/Desktop/FoodRecommendation/artifacts/FoodRecognition/raw_data/public_training_set_release_2.0/images'\n",
    "image_file_val = r'/Users/mac/Desktop/FoodRecommendation/artifacts/FoodRecognition/raw_data/public_validation_set_2.0/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(annotations_file_train, 'r') as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "train_images = pd.DataFrame(train_data['images']).rename(columns={'id': 'image_id'})[['image_id', 'file_name']]\n",
    "train_categories = pd.DataFrame(train_data['categories'])[['id', 'name']].rename(columns={'id': 'category_id'})\n",
    "train_annotations = pd.DataFrame(train_data['annotations'])[['image_id', 'category_id']]\n",
    "\n",
    "train_df = train_annotations.merge(train_categories, on='category_id').merge(train_images, on='image_id')[['file_name', 'name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organized_train_dir = \"../artifacts/FoodRecognition/organized_training_data\"\n",
    "\n",
    "if not os.path.exists(organized_train_dir):\n",
    "    os.makedirs(organized_train_dir)\n",
    "\n",
    "for category in train_df['name'].unique():\n",
    "    category_dir = os.path.join(organized_train_dir, category)\n",
    "    if not os.path.exists(category_dir):\n",
    "        os.makedirs(category_dir)\n",
    "\n",
    "for idx, row in train_df.iterrows():\n",
    "    src = os.path.join(image_file_train, row['file_name'])\n",
    "    dst = os.path.join(organized_train_dir, row['name'], row['file_name'])\n",
    "    if os.path.exists(src): \n",
    "        shutil.copy(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(annotations_file_val, 'r') as file:\n",
    "    val_data = json.load(file)\n",
    "\n",
    "val_images = pd.DataFrame(val_data['images']).rename(columns={'id': 'image_id'})[['image_id', 'file_name']]\n",
    "val_categories = pd.DataFrame(val_data['categories'])[['id', 'name']].rename(columns={'id': 'category_id'})\n",
    "val_annotations = pd.DataFrame(val_data['annotations'])[['image_id', 'category_id']]\n",
    "\n",
    "val_df = val_annotations.merge(val_categories, on='category_id').merge(val_images, on='image_id')[['file_name', 'name']]\n",
    "\n",
    "organized_val_dir = \"../artifacts/FoodRecognition/organized_validation_data\"\n",
    "\n",
    "if not os.path.exists(organized_val_dir):\n",
    "    os.makedirs(organized_val_dir)\n",
    "\n",
    "with open(annotations_file_train, 'r') as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "train_categories = pd.DataFrame(train_data['categories'])[['id', 'name']].rename(columns={'id': 'category_id'})\n",
    "\n",
    "for category in train_categories['name'].unique():\n",
    "    category_dir = os.path.join(organized_val_dir, category)\n",
    "    if not os.path.exists(category_dir):\n",
    "        os.makedirs(category_dir)\n",
    "\n",
    "for idx, row in val_df.iterrows():\n",
    "    src = os.path.join(image_file_val, row['file_name'])\n",
    "    dst = os.path.join(organized_val_dir, row['name'], row['file_name'])\n",
    "    if os.path.exists(src): \n",
    "        shutil.copy(src, dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classes in validation set:\", len(os.listdir(organized_val_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Directory Structure:\")\n",
    "for root, dirs, files in os.walk(image_file_train):\n",
    "    print(root, \"contains\", len(files), \"files\")\n",
    "\n",
    "print(\"Validation Directory Structure:\")\n",
    "for root, dirs, files in os.walk(image_file_val):\n",
    "    print(root, \"contains\", len(files), \"files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,             \n",
    "    rotation_range=20,           \n",
    "    width_shift_range=0.2,      \n",
    "    height_shift_range=0.2,      \n",
    "    shear_range=0.2,             \n",
    "    zoom_range=0.2,              \n",
    "    horizontal_flip=True,        \n",
    "    fill_mode='nearest',\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    organized_train_dir,\n",
    "    target_size=(224, 224),     # GOOGLENET InputSize\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    color_mode='rgb',  \n",
    "    interpolation='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    organized_val_dir,\n",
    "    target_size=(224, 224),      \n",
    "    batch_size=32, \n",
    "    class_mode='categorical'     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dense, Dropout, Input, concatenate\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(224, 224, 3))\n",
    "\n",
    "# Initial convolution and pooling layers\n",
    "x = Conv2D(64, (7, 7), strides=(2, 2), padding='same', activation='relu')(input_layer)\n",
    "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "x = Conv2D(64, (1, 1), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "x = Conv2D(192, (3, 3), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "# Inception module 3a\n",
    "branch1_3a = Conv2D(64, (1, 1), padding='same', activation='relu')(x)\n",
    "\n",
    "branch2_3a = Conv2D(96, (1, 1), padding='same', activation='relu')(x)\n",
    "branch2_3a = Conv2D(128, (3, 3), padding='same', activation='relu')(branch2_3a)\n",
    "\n",
    "branch3_3a = Conv2D(16, (1, 1), padding='same', activation='relu')(x)\n",
    "branch3_3a = Conv2D(32, (5, 5), padding='same', activation='relu')(branch3_3a)\n",
    "\n",
    "branch4_3a = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "branch4_3a = Conv2D(32, (1, 1), padding='same', activation='relu')(branch4_3a)\n",
    "\n",
    "x = concatenate([branch1_3a, branch2_3a, branch3_3a, branch4_3a], axis=-1)\n",
    "\n",
    "# Inception module 3b\n",
    "branch1_3b = Conv2D(128, (1, 1), padding='same', activation='relu')(x)\n",
    "\n",
    "branch2_3b = Conv2D(128, (1, 1), padding='same', activation='relu')(x)\n",
    "branch2_3b = Conv2D(192, (3, 3), padding='same', activation='relu')(branch2_3b)\n",
    "\n",
    "branch3_3b = Conv2D(32, (1, 1), padding='same', activation='relu')(x)\n",
    "branch3_3b = Conv2D(96, (5, 5), padding='same', activation='relu')(branch3_3b)\n",
    "\n",
    "branch4_3b = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "branch4_3b = Conv2D(64, (1, 1), padding='same', activation='relu')(branch4_3b)\n",
    "\n",
    "x = concatenate([branch1_3b, branch2_3b, branch3_3b, branch4_3b], axis=-1)\n",
    "\n",
    "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "# Inception module 4a\n",
    "branch1_4a = Conv2D(192, (1, 1), padding='same', activation='relu')(x)\n",
    "\n",
    "branch2_4a = Conv2D(96, (1, 1), padding='same', activation='relu')(x)\n",
    "branch2_4a = Conv2D(208, (3, 3), padding='same', activation='relu')(branch2_4a)\n",
    "\n",
    "branch3_4a = Conv2D(16, (1, 1), padding='same', activation='relu')(x)\n",
    "branch3_4a = Conv2D(48, (5, 5), padding='same', activation='relu')(branch3_4a)\n",
    "\n",
    "branch4_4a = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "branch4_4a = Conv2D(64, (1, 1), padding='same', activation='relu')(branch4_4a)\n",
    "\n",
    "x = concatenate([branch1_4a, branch2_4a, branch3_4a, branch4_4a], axis=-1)\n",
    "\n",
    "# Auxiliary output\n",
    "aux1 = AveragePooling2D((5, 5), strides=(3, 3))(x)\n",
    "aux1 = Conv2D(128, (1, 1), padding='same', activation='relu')(aux1)\n",
    "aux1 = Flatten()(aux1)\n",
    "aux1 = Dense(1024, activation='relu')(aux1)\n",
    "aux1 = Dropout(0.7)(aux1)\n",
    "aux1 = Dense(498, activation='softmax')(aux1)  # Updated to match main output\n",
    "\n",
    "# Inception module 4b\n",
    "branch1_4b = Conv2D(160, (1, 1), padding='same', activation='relu')(x)\n",
    "\n",
    "branch2_4b = Conv2D(112, (1, 1), padding='same', activation='relu')(x)\n",
    "branch2_4b = Conv2D(224, (3, 3), padding='same', activation='relu')(branch2_4b)\n",
    "\n",
    "branch3_4b = Conv2D(24, (1, 1), padding='same', activation='relu')(x)\n",
    "branch3_4b = Conv2D(64, (5, 5), padding='same', activation='relu')(branch3_4b)\n",
    "\n",
    "branch4_4b = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "branch4_4b = Conv2D(64, (1, 1), padding='same', activation='relu')(branch4_4b)\n",
    "\n",
    "x = concatenate([branch1_4b, branch2_4b, branch3_4b, branch4_4b], axis=-1)\n",
    "\n",
    "# Inception module 5a\n",
    "branch1_5a = Conv2D(256, (1, 1), padding='same', activation='relu')(x)\n",
    "\n",
    "branch2_5a = Conv2D(160, (1, 1), padding='same', activation='relu')(x)\n",
    "branch2_5a = Conv2D(320, (3, 3), padding='same', activation='relu')(branch2_5a)\n",
    "\n",
    "branch3_5a = Conv2D(32, (1, 1), padding='same', activation='relu')(x)\n",
    "branch3_5a = Conv2D(128, (5, 5), padding='same', activation='relu')(branch3_5a)\n",
    "\n",
    "branch4_5a = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "branch4_5a = Conv2D(128, (1, 1), padding='same', activation='relu')(branch4_5a)\n",
    "\n",
    "x = concatenate([branch1_5a, branch2_5a, branch3_5a, branch4_5a], axis=-1)\n",
    "\n",
    "# Final layers\n",
    "x = AveragePooling2D((7, 7))(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Flatten()(x)\n",
    "output = Dense(498, activation='softmax')(x)  # Main output\n",
    "\n",
    "# Build and compile the model\n",
    "model = Model(inputs=input_layer, outputs=[output, aux1])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=['categorical_crossentropy', 'categorical_crossentropy'],  # Loss for both outputs\n",
    "    metrics=[['accuracy'], ['accuracy']]  # Metrics for both outputs\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_generator.samples // val_generator.batch_size,\n",
    "    epochs=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
